{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f537dd43-d39c-471b-a54e-07f1c3ddb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "import IPython.display as ipd\n",
    "\n",
    "import re, yaml, numpy as np, tiktoken\n",
    "import ollama\n",
    "import os, json\n",
    "import os, sys\n",
    "import psycopg\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "813abd7e-5309-44c9-b186-b303ce069216",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM = \"gemma3:4b\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI Oral Board Examiner. Answer concisely and clinically. \n",
    "Use the provided CONTEXT. If uncertain, say so.\"\"\"\n",
    "\n",
    "SYSTEM_EXAMINER = \"\"\"You are the EXAMINER in a surgical oral boards simulation.\n",
    "Follow these rules strictly:\n",
    "- Ask EXACTLY ONE concise clinical question per turn (<= 100 words).\n",
    "- Do not coach unless the candidate types /hint. Do not answer for the candidate.\n",
    "- Use only facts present in CONTEXT; if information is not present in the base case, say: \"Information not provided in the base case.\"\n",
    "- Keep a professional, neutral tone. Be deterministic.\n",
    "- During /grade provide: brief strengths, brief deficits, score 0–10, and pass/fail with rationale tied to case facts.\n",
    "\"\"\"\n",
    "\n",
    "BASE_PATH = Path.home() / \"OneDrive/Desktop/SoftwareDocuments/baseCase.md\"\n",
    "BASE_MD = Path(\"C:\\\\Users\\\\jmcam\\\\OneDrive\\\\Desktop\\\\SoftwareDocuments\\\\baseCase.md\").read_text(encoding=\"utf-8\")\n",
    "doc_text = BASE_MD\n",
    "\n",
    "SYSTEM_EXAMINER = \"\"\"You are the EXAMINER in a surgical oral boards simulation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a6c2f01-6802-4891-9d0b-a5688420c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def tokens(s): return len(enc.encode(s))\n",
    "\n",
    "def split_by_headings(md: str):\n",
    "    parts = re.split(r\"(?m)^##\\s+|^###\\s+\", md)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "#Makes chunks <= max_tokens with a 50 overlap so context is not lost\n",
    "def smart_chunk(md: str, max_tokens=500, overlap=50):\n",
    "    raw = split_by_headings(md)\n",
    "    chunks = []\n",
    "    for part in raw:\n",
    "        if tokens(part) <= max_tokens:\n",
    "            chunks.append(part)\n",
    "        else:\n",
    "            words = part.split()\n",
    "            cur, cur_tokens = [], 0\n",
    "            for w in words:\n",
    "                tw = tokens(w + \" \")\n",
    "                if cur_tokens + tw > max_tokens:\n",
    "                    chunks.append(\" \".join(cur))\n",
    "                    # token-overlap to preserve context at boundaries\n",
    "                    back = enc.decode(enc.encode(\" \".join(cur))[-overlap:])\n",
    "                    cur = back.split() if back else []\n",
    "                    cur_tokens = tokens(\" \".join(cur))\n",
    "                cur.append(w)\n",
    "                cur_tokens += tw\n",
    "            if cur:\n",
    "                chunks.append(\" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "chunks = smart_chunk(doc_text, max_tokens=450, overlap=60)\n",
    "\n",
    "def embed_batch(texts):\n",
    "    vecs = []\n",
    "    for t in texts:\n",
    "        e = ollama.embeddings(model=EMBED_MODEL, prompt=t)[\"embedding\"]\n",
    "        vecs.append(np.array(e, dtype=np.float32))\n",
    "    arr = np.vstack(vecs) if vecs else np.zeros((0,1), dtype=np.float32)\n",
    "    # normalize for cosine\n",
    "    arr /= (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-9)\n",
    "    return arr\n",
    "\n",
    "chunk_vecs = embed_batch(chunks)\n",
    "\n",
    "def retrieve(query: str, k=5):\n",
    "    q = np.array(ollama.embeddings(model=EMBED_MODEL, prompt=query)[\"embedding\"], dtype=np.float32)\n",
    "    q /= (np.linalg.norm(q) + 1e-9)\n",
    "    sims = chunk_vecs @ q\n",
    "    idx = np.argsort(-sims)[:k]\n",
    "    return [(float(sims[i]), chunks[i]) for i in idx]\n",
    "\n",
    "\n",
    "#ask next question using retrieved context based on the last candidate utterance\n",
    "#and (if available) the last examiner question to maintain thread\n",
    "def examiner_question(history, k_ctx=4):\n",
    "    last_user = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"user\"), \"acute appendicitis adult\")\n",
    "    last_exam_q = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"assistant\"), \"\")\n",
    "    retrieval_query = (last_exam_q + \" \" + last_user).strip()\n",
    "\n",
    "    ctx = retrieve(retrieval_query, k=k_ctx)\n",
    "    context_block = \"\\n\\n---\\n\".join([c for _, c in ctx]) if ctx else \"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_EXAMINER},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"CONTEXT:\\n\"\n",
    "                f\"{context_block}\\n\\n\"\n",
    "                f\"CANDIDATE PREVIOUS: {last_user}\\n\"\n",
    "                \"Now ask exactly ONE next oral-boards question. Do not reveal answers.\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    out = ollama.chat(model=LLM, messages=messages)[\"message\"][\"content\"].strip()\n",
    "    return out\n",
    "\n",
    "\n",
    "def examiner_hint(history):\n",
    "    last_user = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"user\"), \"\")\n",
    "    ctx = retrieve(last_user or \"acute appendicitis adult\", k=3)\n",
    "    context_block = \"\\n\\n---\\n\".join([c for _, c in ctx]) if ctx else \"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_EXAMINER},\n",
    "        {\"role\":\"user\", \"content\":\n",
    "         f\"CONTEXT:\\n{context_block}\\n\"\n",
    "         f\"Candidate said: {last_user}\\nProvide ONE short hint (<=25 words).\"}\n",
    "    ]\n",
    "    return ollama.chat(model=LLM, messages=messages)[\"message\"][\"content\"].strip()\n",
    "\n",
    "def examiner_grade(history):\n",
    "    last_user = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"user\"), \"\")\n",
    "    ctx = retrieve(last_user or \"evaluation rubric\", k=3)\n",
    "    context_block = \"\\n\\n---\\n\".join([c for _, c in ctx]) if ctx else \"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_EXAMINER},\n",
    "        {\"role\":\"user\", \"content\":\n",
    "         f\"CONTEXT:\\n{context_block}\\n\"\n",
    "         f\"Evaluate the candidate's last answer:\\n\\\"\\\"\\\"{last_user}\\\"\\\"\\\"\\n\"\n",
    "         \"Return: score 0–10, 2–3 bullet strengths, 2–3 bullet deficits, pass/fail with rationale tied to case facts.\"}\n",
    "    ]\n",
    "    return ollama.chat(model=LLM, messages=messages)[\"message\"][\"content\"].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d721d835-0074-4a3c-853e-34dbe9a705e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim():\n",
    "    print(\"Oral Boards Simulator\\n\"\n",
    "          \"Commands: /start, /next, /hint, /grade, /end\\n\"\n",
    "          \"You are the CANDIDATE. The AI is the EXAMINER.\\n\")\n",
    "    history = []\n",
    "    started = False\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_in = input(\"> \").strip()\n",
    "\n",
    "            if user_in.lower() == \"/end\":\n",
    "                print(\"Session ended.\")\n",
    "                break\n",
    "\n",
    "            if user_in.lower() == \"/start\":\n",
    "                started = False\n",
    "                history = []\n",
    "                q = examiner_question(history)\n",
    "                print(f\"\\nExaminer: {q}\\n\")\n",
    "                history.append({\"role\":\"assistant\",\"content\":q})\n",
    "                continue\n",
    "\n",
    "            if not started:\n",
    "                print('Type \"/start\" to begin.')\n",
    "                continue\n",
    "\n",
    "            if user_in.lower() == \"/next\":\n",
    "                # Advance flow: simply ask next question; retrieval uses last turn\n",
    "                q = examiner_question(history)\n",
    "                print(f\"\\nExaminer: {q}\\n\")\n",
    "                history.append({\"role\":\"assistant\",\"content\":q})\n",
    "                continue\n",
    "\n",
    "            if user_in.lower() == \"/hint\":\n",
    "                h = examiner_hint(history)\n",
    "                print(f\"\\nHint: {h}\\n\")\n",
    "                continue\n",
    "\n",
    "            if user_in.lower() == \"/grade\":\n",
    "                g = examiner_grade(history)\n",
    "                print(f\"\\nGrading:\\n{g}\\n\")\n",
    "                continue\n",
    "\n",
    "            # Candidate's answer\n",
    "            history.append({\"role\":\"user\",\"content\":user_in})\n",
    "            if len(history) > 24:\n",
    "                history = history[-24:]  # keep context compact\n",
    "\n",
    "            # Next examiner question\n",
    "            q = examiner_question(history)\n",
    "            print(f\"\\nExaminer: {q}\\n\")\n",
    "            history.append({\"role\":\"assistant\",\"content\":q})\n",
    "\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nSession interrupted.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d941d-264d-4bcc-9c0b-4933109e0307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Loading baseCase from: {BASE_PATH}\")\n",
    "    run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595f356-875f-483c-9bdb-923109f442cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for later use and was just for testing text to speach\n",
    "\n",
    "\n",
    "def speak_text(text: str, out_path: str = \"answer.wav\", speaker: str | None = None):\n",
    "    kwargs = {}\n",
    "    if speaker is not None: #multi-speaker models\n",
    "        kwargs[\"speaker\"]=speaker\n",
    "    tts.tts_to_file(text=text, file_path=out_path, **kwargs)\n",
    "    return ipd.Audio(out_path)\n",
    "\n",
    "def chat_and_speak(prompt: str, out_path=\"answer.wav\", speaker=None):\n",
    "    resp = ollama.chat(model=LLM, messages=[{\"role\":\"user\",\"content\": prompt}])\n",
    "    text = resp[\"message\"][\"content\"]\n",
    "    print(\"LLM:\", text)\n",
    "    return speak_text(text, out_path, speaker)\n",
    "\n",
    "# use it:\n",
    "chat_and_speak(\"Give three key symptoms of diverticulitis. No disclaimers, no special characters, give short sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38e0b6e6-a06d-4c5e-a453-4659dec741fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: C:\\Users\\jmcam\\.venv-tts\\Scripts\\python.exe\n",
      "Found git at: C:\\Program Files\\Git\\cmd\\git.exe\n",
      "git --version -> git version 2.51.0.windows.1\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, glob\n",
    "\n",
    "# Candidate install locations (system + per-user)\n",
    "cands = [\n",
    "    r\"C:\\Program Files\\Git\\cmd\\git.exe\",\n",
    "    r\"C:\\Program Files (x86)\\Git\\cmd\\git.exe\",\n",
    "    r\"%LOCALAPPDATA%\\Programs\\Git\\cmd\\git.exe\",\n",
    "]\n",
    "cands = [os.path.expandvars(p) for p in cands]\n",
    "\n",
    "# Also try to discover git.exe if installed in a nonstandard place\n",
    "cands += glob.glob(r\"C:\\Program Files\\Git\\cmd\\git.exe\")\n",
    "cands += glob.glob(os.path.expandvars(r\"%LOCALAPPDATA%\\Programs\\Git\\cmd\\git.exe\"))\n",
    "\n",
    "found = next((p for p in cands if os.path.exists(p)), None)\n",
    "print(\"Kernel:\", sys.executable)\n",
    "print(\"Found git at:\", found)\n",
    "\n",
    "if found:\n",
    "    git_cmd = os.path.dirname(found)\n",
    "    git_bin = git_cmd.replace(r\"\\cmd\", r\"\\bin\")\n",
    "    os.environ[\"PATH\"] += \";\" + git_cmd + \";\" + git_bin\n",
    "    try:\n",
    "        print(\"git --version ->\", subprocess.check_output([\"git\",\"--version\"], text=True).strip())\n",
    "    except Exception as e:\n",
    "        print(\"Tried to add PATH but still failing:\", e)\n",
    "else:\n",
    "    print(\"Could not locate git.exe automatically. If you know the path, run:\")\n",
    "    print(r'import os; os.environ[\"PATH\"] += r\";C:\\Path\\To\\Git\\cmd;C:\\Path\\To\\Git\\bin\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a976c172-1fbf-40a0-a2c6-ad37e2b95246",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git remote set-url origin https://github.com/Jonah-Camacho/ENGR-498.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b95a0ffd-233d-4a98-875f-8b317f19b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/jmcam\n"
     ]
    }
   ],
   "source": [
    "!git rev-parse --show-toplevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdc214f5-4b1f-45cd-b691-7725be1941e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmcam\\Documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmcam\\.venv-tts\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmcam\\Documents\\ENGR-498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ENGR-498'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/jmcam/Documents/ENGR-498\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "# go somewhere you want the clone to live\n",
    "%cd \"C:\\Users\\jmcam\\Documents\"\n",
    "\n",
    "# (optional) if a stale clone exists, delete it\n",
    "!rmdir /S /Q ENGR-498 2>nul\n",
    "\n",
    "# fresh clone and cd into it\n",
    "!git clone https://github.com/Jonah-Camacho/ENGR-498.git\n",
    "%cd ENGR-498\n",
    "\n",
    "# sanity check you’re in the right repo\n",
    "!git rev-parse --show-toplevel\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ec8ddb9-adf2-49ba-8ecd-02876e681d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "!copy \"C:\\Users\\jmcam\\OralBoardAI.ipynb\" \"C:\\Users\\jmcam\\Documents\\ENGR-498\\OralBoardAI.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe8765ca-850b-4380-ac8c-4fc28adddbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmcam\\Documents\\ENGR-498\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\jmcam\\Documents\\ENGR-498\"\n",
    "!git add -- \"OralBoardAI.ipynb\"\n",
    "!git commit -m \"Add OralBoardAI notebook\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a465bfc7-144b-43b9-933f-988add3f01e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmcam\\OralBoardAI.ipynb\n",
      "C:\\Users\\jmcam\\ENGR-498-reset\\ENGR-498\\OralBoardAI.ipynb\n"
     ]
    }
   ],
   "source": [
    "!where /r \"C:\\Users\\jmcam\" OralBoardAI.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2f81b-9968-4107-9034-0ef05f339850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tts)",
   "language": "python",
   "name": "tts310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
